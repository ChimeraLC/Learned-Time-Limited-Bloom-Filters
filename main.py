import argparse
import json
import random
import torch
import numpy as np
import time
from matplotlib import pyplot as plt
import pandas as pd

from runner import Runner
from bloom_filters.sandwich_time_bloom import Learned_AP_Bloom
from bloom_filters.time_bloom import AP_Bloom

# Parse the arguments
def get_args():
    parser = argparse.ArgumentParser(description="Temporary Description")

    # General Args
    parser.add_argument("--seed", type=int, default=0)

    #  Data Args
    parser.add_argument("--common_data_path", type=str, default="data/player-stats.csv") #data from https://github.com/lukearend/osrs-hiscores
    parser.add_argument("--uncommon_data_path", type=str, default="data/false-names.csv") #data should be generated by gen_false_usernames.py
    parser.add_argument("--model_save_path", type=str, default="saved-model")  # The path to save the model to
    parser.add_argument("--retrain_model", type=bool, default = False)  # Whether to retrain the model each time
    parser.add_argument("--max_len", type=int, default = 12)  # Maximum length of usernames
    parser.add_argument("--chars", type=str, default="qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM1234567890_[]#- ") # List of encoded characters in usernames
    parser.add_argument("--data_length", type=int, default = 50000) # Data used for training
    parser.add_argument("--train_ratio", type=float, default=0.7) # Proportion of data used for training

    # Arguments for the test experiments to run
        # Sizes of the updated and trailing age generations
    parser.add_argument("--filter_generations", type=list, default = [[2, 1], [3, 2], [5, 3]])
        # Size of each age generation
    parser.add_argument("--filter_generation_sizes", type=list, default=[2500, 5000, 10000])
        # Desired false-positive rates
    parser.add_argument("--filter_fps", type=list, default = [0.1, 0.05, 0.02, 0.01])
        # The proportion of elements that come from the 
    parser.add_argument("--usuals_proportion", type=list, default = [0.5, 0.8, 0.95])

    # Model Args: Load JSON file
    parser.add_argument('--model_args', type=str, default="args.json")

    args = parser.parse_args()

    # Additionally load hyperparameter values
    with open(args.model_args, 'rt') as f:
        args.__dict__.update(json.load(f))

    return args

# Fix the seed
def seed_all(fixed_seed):
    random.seed(fixed_seed)
    torch.manual_seed(fixed_seed)
    np.random.seed(fixed_seed)

# Train and return the model
def run(args):
    runner = Runner(args)
    runner.train()
    return runner

# Run an experiment with the given parameters
def test(runner, k, l, desired_fp, real_percent, size, args):
    # Print dividing line
    print("---------------------------------------------------------")
    print("Running experiment with k =", k, "l=", l, "fp=", desired_fp, "size=", size, "prop=", real_percent)

    # Initialize filters
    ap_filter = AP_Bloom(k, l, size, desired_fp)
    l_ap_filter = Learned_AP_Bloom(k, l, size, desired_fp, runner)

    # Output size to console
    print("Default filter has bit array of size", ap_filter.get_size())
    print("Default filter has bit array of size", l_ap_filter.get_size()[0], "and model of size", l_ap_filter.get_size()[1])

    # Calculate size of each type
    real_size = int(real_percent * size * (k + l) * 2)
    unreal_size =  (k + l) * 2 * size - real_size

    # Read data from files
    real_data = pd.read_csv(args.common_data_path) # Realistic names
    unreal_data = pd.read_csv(args.uncommon_data_path) # Unrealistic names

    # Take names not used for the training process
    incl_names = np.concatenate((real_data['username'][runner.args.data_length + 20000:
                                                       runner.args.data_length + 20000 + real_size].values,
                            unreal_data['username'][runner.args.data_length + 20000:
                                                    runner.args.data_length + 20000 + unreal_size].values))
    # Shuffle names
    np.random.shuffle(incl_names)

    # Insert first few usernames into filter
    for name in incl_names[:l * size]:
        ap_filter.insert(name)
        l_ap_filter.insert(name)

    # Check inclusion rate of recent usernames
    included = 0
    l_included = 0
    for name in incl_names[:l * size]:
        if (ap_filter.query(name)):
            included += 1
        if (l_ap_filter.query(name)):
            l_included += 1
    # Output to console
    print("Out of", l * size, "elements,", included, "were correctly included in default filter")
    print("Out of", l * size, "elements,", l_included, "were correctly included in learned filter")

    # Insert more usernames
    for name in incl_names[k * size:(k + l) * size]:
        ap_filter.insert(name)
        l_ap_filter.insert(name)

    # Check inclusion rate of 'phased out' usernames
    included = 0
    l_included = 0
    for name in incl_names[:k * size]:
        if (ap_filter.query(name)):
            included += 1
        if (l_ap_filter.query(name)):
            l_included += 1
    # Output to console
    print("Out of the first", k * size, "elements,", included, "were still included in default filter")
    print("Out of the first", k * size, "elements,", l_included, "were still included in learned filter")

    # Check inclusion rate of recent usernames
    included = 0
    l_included = 0
    for name in incl_names[k * size: (k+l) * size]:
        if (ap_filter.query(name)):
            included += 1
        if (l_ap_filter.query(name)):
            l_included += 1
    # Output to console
    print("Out of", l * size, "recent elements,", included, "were correctly included in default filter")
    print("Out of", l * size, "recent elements,", l_included, "were correctly included in learned filter")

    # Check false positive rate on similarly distributed values
    excl_names = incl_names[(k + l) * size:]
    included = 0
    l_included = 0
    # Track query time
    start_time = time.time()
    for name in excl_names:
        if (ap_filter.query(name)):
            included += 1
    ap_time = time.time() - start_time
    start_time = time.time()
    for name in excl_names:
        if (l_ap_filter.query(name)):
            l_included += 1
    l_ap_time = time.time() - start_time
    # Calculate fp rate
    ap_fp = included/len(excl_names)
    l_ap_fp = l_included/len(excl_names)
    # Calculate average query time
    ap_time /= len(excl_names)
    l_ap_time /= len(excl_names)
    # Output to console
    print("False positive rate of", ap_fp, "among distributed usernames in default filter, average query time of", ap_time)
    print("False positive rate of", l_ap_fp, "among distributed usernames in learned filter, average query time of", l_ap_time)

    # Check false positive rate on only uncommon usernames
    unreal_names = unreal_data['username'][runner.args.data_length: runner.args.data_length + 20000].values
    included = 0
    l_included = 0
    for name in unreal_names:
        if (ap_filter.query(name)):
            included += 1
        if (l_ap_filter.query(name)):
            l_included += 1
    # Calculate fp rate
    ap_fp_un = included/len(unreal_names)
    l_ap_fp_un = l_included/len(unreal_names)
    # Output to console
    print("There is a false positive rate of", ap_fp_un, "among uncommon usernames in default filter")
    print("There is a false positive rate of", l_ap_fp_un, "among uncommon usernames in learned filter")

    # Return fp rates
    return ap_fp, l_ap_fp, ap_fp_un, l_ap_fp_un

# Main process of script
if __name__ == "__main__":
    # Parse arguments
    args = get_args()

    # Seed
    seed_all(args.seed)

    # Train/Load model based on username dataset
    runner = run(args)


    # Run different experiments
    for prop in args.usuals_proportion:
        # Create plot for each proportion
        fig, axes = plt.subplots(nrows=len(args.filter_generation_sizes), ncols=len(args.filter_generations))
        fig.tight_layout()
        fig.suptitle("Normal username proportion of " + str(prop))

        # Select different values of k
        index = 0
        # Run the experiment for each pair k, l
        for gen_ind in range(len(args.filter_generations)):
            k, l = args.filter_generations[gen_ind]
            # Run the experiment for each generation size
            for size_ind in range(len(args.filter_generation_sizes)):
                size = args.filter_generation_sizes[size_ind]
                index += 1

                # Get the corresponding plot
                ax = axes[size_ind, gen_ind]
                ax.set_xlabel("Aimed FP")
                ax.set_ylabel("Actual FP")
                # Labeling each plot
                ax.set_title("k="+str(k)+" l="+ str(l)+" size="+str(size))

                # Store the fp rates
                ap_fps = []
                l_ap_fps = []
                ap_fps_un = []
                l_ap_fps_un = []

                # Run the experiment for each fp rate
                for desired_fp in args.filter_fps:
                    ap_fp, l_ap_fp, ap_fp_un, l_ap_fp_un = test(runner, k, l, desired_fp, prop, size, args)
                    # Store the results
                    ap_fps.append(ap_fp)
                    l_ap_fps.append(l_ap_fp)
                    ap_fps_un.append(ap_fp_un)
                    l_ap_fps_un.append(l_ap_fp_un)
                    
                # Plot the given results
                ax.plot(args.filter_fps, ap_fps, label = "Default Filter FP Rate")
                ax.plot(args.filter_fps, l_ap_fps, label = "Learned Filter FP Rate")
                ax.plot(args.filter_fps, ap_fps_un, label = "Default FP on Uncommon Usernames")
                ax.plot(args.filter_fps, l_ap_fps_un, label = "Learned FP on Uncommon Usernames")

                # Include labels
                if index == 1:
                    ax.legend()

    # Display plots
    print("---------------------------------------------------------")
    print("Experiments complete, displaying graphs")
    plt.show()